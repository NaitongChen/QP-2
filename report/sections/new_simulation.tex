% !TEX root = ../main.tex

% Summary section

\section{Reproduced and revised simulation studies}\label{sec:newsimulation}
\begin{table}[]
\centering
\begin{tabular}{llllll}
       &         & Lasso    & 5\% / 95\% pc & RA Lasso  & 5\% / 95\% pc \\ \hline
N(0,4) & L2-loss & 4.30 & (2.70, 6.91)     & 4.58  & (2.70, 7.76)     \\
       & L1-loss & 28.15 & (17.32, 45.93)     & 29.81  & (17.17, 50.34)     \\
       & FP      & 67.53    & (58.90, 74.10)             & 64.01     & (55.95, 71.00)           \\
       & FN      & 0.07     & (0,0)                    & 0.10       & (0,1)                    \\
       & RG-L2   &          &                          & 0.95  & (0.74, 1.08)   \\
       & RG-L1   &          &                          & 0.93  & (0.73, 1.08)   \\ \hline
2t3    & L2-loss & 5.08 & (2.59, 9.18)     & 5.10  & (2.65, 9.52)     \\
       & L1-loss & 33.84 & (16.20, 61.74)     & 33.21  & (16.24, 62.74)     \\
       & FP      & 69.05    & (60.00, 77.05)              & 63.29     & (55.95, 71.00)           \\
       & FN      & 0.15     & (0, 1)                   & 0.25      & (0,2)                    \\
       & RG-L2   &          &                          & 1.06 & (0.80, 1.62)   \\
       & RG-L1   &          &                          & 1.05  & (0.77, 1.58)     \\ \hline
MixN   & L2-loss & 8.86 & (6.86, 11.31)    & 8.94  & (6.76, 11.20)    \\
       & L1-loss & 54.78 & (40.23, 71.51)     & 54.34  & (40.61, 70.38)     \\
       & FP      & 54.34    & (47, 62)                 & 52.41     & (45.95, 60.00)           \\
       & FN      & 1.59     & (0, 5)                   & 1.79      & (0.00, 5.05)             \\
       & RG-L2   &          &                          & 1.01 & (0.92, 1.09)   \\
       & RG-L1   &          &                          & 0.99  & (0.90, 1.07)  
\end{tabular}
\caption{Simulation results for lasso and RA lasso under homoscedastic model}\label{tab:reproduce}
\end{table}
In this section, we begin by reproducing a subset of the simulation studies in Table 2 of \citet{fan2017estimation}. We also address some of the practical concerns mentioned in \cref{sec:relevance} and incorporate them to a revised simulation study with an expanded set of error distributions and regression estimators. All of the code used to conduct the simulations can be found at \url{https://github.com/NaitongChen/QP-2}.

$ $\newline
\cref{tab:reproduce} shows the mean error metrics for three of the error distributions from Table 2 of \citet{fan2017estimation} using LASSO and RA-lasso. Here we follow the setup in the original code of the paper as much as we can. Specifically, to perform hyperparameter tuning, a 2-dimensional grid of $15$ equally spaced $\alpha$ values ranging from $0.0001$ to $1$ and $30$ equally spaced $\lambda$ values ranging from $0.0001$ to $1.5$ are used. Since the sequence of $\lambda$ values used to tune LASSO is not provided by the authors, a sequence of $200$ values ranging from $0.00001$ to $10$ is used. This sequence is chosen because the number of candidate hyperparameter values is on the same order of magnitude of the total sets of hyperparameters tested for RA-lasso. At the same time, the spacing between the candidate $\lambda$ values for LASSO and RA-lasso are both kept to be around $0.05$. Note that to solve the (convex) optimization problem in order to get the RA-lasso estimates, the \texttt{CVXR} package in \texttt{R} is used. This is an equivalent package of \texttt{CVX} in \texttt{MATLAB}, which is used in the code to generate the results in the paper. The \texttt{glmnet} package is used to obtain the LASSO estimates. In addition to the mean values, the corresponding $5^\text{th}$ and $95^\text{th}$ percentiles as a measure of uncertainty are also reported. We note that, similar to Table 2 in \citet{fan2017estimation}, LASSO and RA-lasso perform similarly across all three scenarios.

\begin{table}[]
\centering
\begin{tabular}{llllllll}
       &         & Lasso  & 5\% / 95\% pc & RA Lasso & 5\% / 95\% pc & PENSE & 5\% / 95\% pc \\ \hline
N(0,4) & L2-loss & 5.8    & (4.42, 7.63)     & 5.26     & (3.31, 8.89)     & 10.12 & (4.05, 12.97)    \\
       & L1-loss & 57.17  & (30.56, 82.48)   & 33.22    & (22.19, 63.80)   & 64.18 & (30.12, 80.14)   \\
       & FP      & 259.1  & (28.00, 379.05)  & 60.85    & (29, 81)         & 69.1  & (64.00, 77.05)   \\
       & FN      & 0.05   & (0.00, 0.05)     & 0.1      & (0, 1)           & 5     & (0.0, 11.1)      \\
       & RG-L2   &        &               & 2.14     & (0.89, 4.01)     & 1.06  & (0.45, 2.61)     \\
       & RG-L1   &        &               & 1.3      & (0.62, 2.01)    & 0.64  & (0.31, 1.40)     \\ \hline
t2     & L2-loss & 5.98   & (4.65, 7.37)     & 5        & (2.47, 8.48)     & 8.8   & (2.72, 12.57)    \\
       & L1-loss & 57.07  & (30.23, 82.01)   & 31.76    & (17.40, 59.04)   & 58.02 & (19.53, 80.68)   \\
       & FP      & 255.95 & (43.15, 379.05)  & 61.85    & (42.05, 80.00)   & 73.85 & (65.7, 79.0)     \\
       & FN      & 0.25   & (0.00, 0.25)     & 0.25     & (0.00, 1.15)     & 3.35  & (0.0, 9.1)       \\
       & RG-L2   &        &               & 2.36     & (0.83, 4.42)     & 1.34  & (0.42, 4.02)     \\
       & RG-L1   &        &               & 1.53     & (0.71, 3.02)     & 0.9   & (0.36, 2.44)     \\ \hline
MixN   & L2-loss & 9.44   & (7.12, 12.24)    & 10.58    & (7.54, 13.35)    & 12.66 & (11.54, 13.54)   \\
       & L1-loss & 67.39  & (46.14, 93.34)   & 61.08    & (45.98, 85.75)   & 77.54 & (60.58, 91.52)   \\
       & FP      & 140.25 & (3, 379)         & 42.35    & (6.75, 80.10)    & 54.25 & (1.00, 81.15)    \\
       & FN      & 3.55   & (0, 13)          & 5.3      & (0.0, 13.3)      & 10.2  & (4, 20)          \\
       & RG-L2   &        &               & 1.12     & (0.84, 1.74)     & 0.86  & (0.48, 1.16)     \\
       & RG-L1   &        &               & 0.87     & (0.62, 1.07)     & 0.69  & (0.46, 0.98)     \\ \hline
Lev1   & L2-loss & 4.19   & (1.65, 6.75)     & 4.53     & (1.63, 12.48)    & 7.73  & (2.10, 13.47)    \\
       & L1-loss & 36.35  & (11.09, 75.37)   & 27.76    & (10.9, 61.6)     & 49.23 & (13.15, 77.11)   \\
       & FP      & 174.4  & (61.20, 378.05)  & 60.8     & (20.65, 79.05)   & 65.45 & (17.15, 74.20)   \\
       & FN      & 0.7    & (0.0, 0.7)       & 1.4      & (0.00, 12.15)    & 3.3   & (0.00, 17.15)    \\
       & RG-L2   &        &               & 1.56     & (0.61, 3.52)     & 0.84  & (0.19, 2.03)     \\
       & RG-L1   &        &               & 1.14     & (0.43, 1.96)     & 0.59  & (0.20, 1.21)     \\ \hline
Lev2   & L2-loss & 5.3    & (2.99, 6.73)     & 2.92     & (1.44, 7.07)     & 7.9   & (2.03, 13.52)    \\
       & L1-loss & 58.46  & (21.25, 78.36)   & 19.24    & (9.47, 38.83)    & 52.35 & (14.40, 88.31)   \\
       & FP      & 329.75 & (77.60, 379.05)  & 67.95    & (51.00, 79.05)   & 74.65 & (67.7, 80.0)     \\
       & FN      & 0      & (0, 0)           & 0        & (0, 0)           & 3     & (0.00, 11.05)    \\
       & RG-L2   &        &               & 4.61     & (0.92, 10.29)    & 1.63  & (0.50, 4.58)     \\
       & RG-L1   &        &               & 2.88     & (0.86, 6.56)     & 0.99  & (0.34, 2.81)    
\end{tabular}
\caption{Additonal simulation results for lasso, RA lasso, and PENSE}\label{tab:revised}
\end{table}

$ $\newline
We now revise the simulation study to address some of the issues discussed in \cref{sec:relevance}. Firstly, in addition to LASSO and RA-lasso, we also add another robust regression estimator PENSE, as a competitor to RA-lasso, to the comparison \citep{freue2019robust}. Furthermore, instead of conducting hyperparameter tuning using independent validation sets and the L2-loss between the true regression parameters and the estimates, we use a 5-fold cross validation with the mean-squared error between the true and fitted responses.

$ $\newline
In terms of the sequence of hyperparameters, instead of setting the range of $\alpha$ and $\lambda$ values somewhat arbitrarily ($0.0001$ to $1$ and $0.0001$ to $1.5$, respectively), as is done in the authors' original code, we follow the guideline in Section 2 of \cref{sec:relevance} and use a sequence of hyperparameters that are equidistant in log scale. Specifically, $15$ $\lambda$ values ranging from $e^{-15}$ to $e^2$ are used for LASSO and PENSE, and the same sequence of $\alpha$ and $\lambda$ values ranging from $e^{-10}$ to $e^2$ are used for RA-lasso. These values are chosen so that the minimum value in each sequence is as small as possible without causing numerical issues. This is a typical approach given any problem in practice: by setting the sequence to be eqidistant in log scale, we can explore hyperparameters in a wide range of orders of magnitude.

$ $\newline
We also expand the simulation study by including two scenarios with various degrees of high leverage observations (unusual covariates) to check the robusteness of RA-lasso. We follow the procedure in \citet{maronna2011robust} and contaminate $10\%$ of the observations in each dataset. Specifically, we replace the predictors $x_i$ with
\begin{align}
\tilde{x}_i = \eta_i + \frac{k_\text{lev}}{\sqrt{a^Ta}}a, \label{eq:perturb}
\end{align}
where $\eta_i \sim N(0, 0.1^2I_p)$ and $a = \tilde{a} - \frac{1}{p}\tilde{a}^T\bm{1}_p$ with the entries of $\tilde{a}$ following $\text{Unif}(\{-1, 1\})$. The error distributions in these cases are set to $N(0,1)$. We also change the error distribution in the second scenario from $2t_3$ to $t_2$ in order to make the heavy-tailedness more pronounced. Finally, instead of centring the error distributions by their empirical average, we centre them using their analytical mean.

$ $\newline
\cref{tab:revised} shows the mean error metrics and their $5^\text{th}$ and $95^\text{th}$ percentiles for each scenario of the revised simulation study. Note that \texttt{lev1} and \texttt{lev2} in the table correspond to $k_\text{lev}=1$ and $k_\text{lev}=2$ in \cref{eq:perturb}, respectively. Also note that due to computational constraints, these values are based on $20$ instead of $100$ trials. We see that across all five scenarios, RA-lasso achieves a similar level of L1 and L2 loss, if not better, compared to LASSO. However, RA-lasso is able to achieve this level of error without including many noise predictors. In other words, it has much lower false positive rates than LASSO. In fact, RA-lasso appears to have a similar level of FP rates compared to PENSE across all five scenarios, indicating a similar level of robustness compared to PENSE. We note that relatively high L1 and L2 loss and larger variances of PENSE relative to LASSO and RA-lasso can be due to the loss of efficiency caused by not directly incorporating the squared loss in the objective.
